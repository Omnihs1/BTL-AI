{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import YOLOv3\n",
    "from library import *\n",
    "from config import learning_rate, ANCHORS, device, s\n",
    "from utils import load_checkpoint, convert_cells_to_bboxes, plot_image, plot_custom_image\n",
    "from dataset import Dataset\n",
    "from augment import test_transform\n",
    "from loss import YOLOLoss\n",
    "from metrics import nms, mean_average_precision\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Loading checkpoint\n",
      "Hoan thanh checkpoint model\n"
     ]
    }
   ],
   "source": [
    "# Check point file\n",
    "checkpoint_file = \"save model/checkpoint_custom_final.pth.tar\"\n",
    "# Setting the load_model to True\n",
    "load_model = True\n",
    "  \n",
    "# Defining the model, optimizer, loss function and scaler\n",
    "model = YOLOv3(num_classes=3).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "loss_fn = YOLOLoss()\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "multiprocessing.freeze_support()\n",
    "# Loading the checkpoint\n",
    "if load_model:\n",
    "    load_checkpoint(checkpoint_file, model, optimizer, learning_rate)\n",
    "\n",
    "print(\"Hoan thanh checkpoint model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the test dataset and data loader\n",
    "test_dataset = Dataset(\n",
    "    csv_file=\"custom data/test.csv\",\n",
    "    image_dir=\"custom data/dataset_resized/dataset_resized/Img\",\n",
    "    label_dir=\"custom data/dataset_resized/dataset_resized/Label\",\n",
    "    anchors=ANCHORS,\n",
    "    transform=test_transform,\n",
    "    num_classes=3\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size = 8,\n",
    "    num_workers = 2,\n",
    "    shuffle = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a sample image from the test data loader\n",
    "pred_boxes = []\n",
    "for (x, y) in test_loader:\n",
    "    x = x.to(device)\n",
    "    with torch.no_grad():\n",
    "        # Getting the model predictions\n",
    "        output = model(x)\n",
    "        # Getting the bounding boxes from the predictions\n",
    "        bboxes = [[] for _ in range(x.shape[0])]\n",
    "        anchors = (\n",
    "                torch.tensor(ANCHORS)\n",
    "                    * torch.tensor(s).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)\n",
    "                ).to(device)\n",
    "        # Getting bounding boxes for each scale\n",
    "        for i in range(3):\n",
    "            batch_size, A, S, _, _ = output[i].shape\n",
    "            anchor = anchors[i]\n",
    "            boxes_scale_i = convert_cells_to_bboxes(\n",
    "                                output[i], anchor, s=S, is_predictions=True\n",
    "                            )\n",
    "            for idx, (box) in enumerate(boxes_scale_i):\n",
    "                bboxes[idx] += box\n",
    "    model.train()\n",
    "    # Create figure and axes\n",
    "    # Plotting the image with bounding boxes for each image in the batch\n",
    "    for i in range(batch_size):\n",
    "        # Applying non-max suppression to remove overlapping bounding boxes\n",
    "        nms_boxes = nms(bboxes[i], iou_threshold=0.3, threshold=0.6)\n",
    "        pred_boxes.append(nms_boxes)\n",
    "        plot_custom_image(x[i].permute(1,2,0).detach().cpu(), nms_boxes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.7091950178146362,\n",
       " 0.470702588558197,\n",
       " 0.3809179365634918,\n",
       " 0.44609999656677246,\n",
       " 0.38880646228790283]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_boxes[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_boxes = []\n",
    "# Creating a dataset object\n",
    "dataset = Dataset(\n",
    "    csv_file=\"custom data/test.csv\",\n",
    "    image_dir=\"custom data/dataset_resized/dataset_resized/Img\",\n",
    "    label_dir=\"custom data/dataset_resized/dataset_resized/Label\",\n",
    "    grid_sizes=[13, 26, 52],\n",
    "    anchors=ANCHORS,\n",
    "    transform=test_transform\n",
    ")\n",
    "\n",
    "# Creating a dataloader object\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    ")\n",
    "# Defining the grid size and the scaled anchors\n",
    "GRID_SIZE = [13, 26, 52]\n",
    "scaled_anchors = torch.tensor(ANCHORS) / (\n",
    "    1 / torch.tensor(GRID_SIZE).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)\n",
    ")\n",
    "\n",
    "# Getting a batch from the dataloader\n",
    "for(x, y) in loader:\n",
    "    # Getting the boxes coordinates from the labels\n",
    "    # and converting them into bounding boxes without scaling\n",
    "    boxes = []\n",
    "    for i in range(y[0].shape[1]):\n",
    "        anchor = scaled_anchors[i]\n",
    "        boxes += convert_cells_to_bboxes(\n",
    "                y[i], is_predictions=False, s=y[i].shape[2], anchors=anchor\n",
    "                )[0]\n",
    "\n",
    "    # Applying non-maximum suppression\n",
    "    boxes = nms(boxes, iou_threshold=1, threshold=0.7)\n",
    "    target_boxes.append(boxes)\n",
    "    # Plotting the image with the bounding boxes\n",
    "    plot_custom_image(x[0].permute(1,2,0).to(\"cpu\"), boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.6059927344322205, 0.39271607995033264, 0.3791066110134125, 0.38310161232948303, 0.2940683364868164]\n"
     ]
    }
   ],
   "source": [
    "img_pred_boxes = pred_boxes[0]\n",
    "for box in img_pred_boxes:\n",
    "    print(box)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from metrics import iou\n",
    "def mean_average_precision(\n",
    "    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=20, num_img=35\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates mean average precision \n",
    "\n",
    "    Parameters:\n",
    "        pred_boxes (list): list of lists containing all bboxes with each bboxes\n",
    "        specified as [train_idx, class_prediction, prob_score, x1, y1, x2, y2]\n",
    "        true_boxes (list): Similar as pred_boxes except all the correct ones \n",
    "        iou_threshold (float): threshold where predicted bboxes is correct\n",
    "        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
    "        num_classes (int): number of classes\n",
    "\n",
    "    Returns:\n",
    "        float: mAP value across all classes given a specific IoU threshold \n",
    "    \"\"\"\n",
    "\n",
    "    # list storing all AP for respective classes\n",
    "    average_precisions = []\n",
    "    k = 0\n",
    "    # used for numerical stability later on\n",
    "    epsilon = 1e-6\n",
    "    for i in range(num_img):\n",
    "        img_pred_boxes = pred_boxes[i]\n",
    "        img_true_boxes = true_boxes[i]\n",
    "        print(\"Target boxes\", img_true_boxes)\n",
    "        for c in range(num_classes):\n",
    "            detections = []\n",
    "            ground_truths = []\n",
    "\n",
    "            # Go through all predictions and targets,\n",
    "            # and only add the ones that belong to the\n",
    "            # current class c\n",
    "            for detection in img_pred_boxes:\n",
    "                if detection[0] == c:\n",
    "                    detections.append(detection)\n",
    "\n",
    "            for true_box in img_true_boxes:\n",
    "                if true_box[0] == c:\n",
    "                    ground_truths.append(true_box)\n",
    "            if detections == [] or ground_truths == []:\n",
    "                continue\n",
    "            print(\"Detection\", detections)\n",
    "            print(\"Ground truth\", ground_truths)\n",
    "            k+=1\n",
    "            # find the amount of bboxes for each training example\n",
    "            # Counter here finds how many ground truth bboxes we get\n",
    "            # for each training example, so let's say img 0 has 3,\n",
    "            # img 1 has 5 then we will obtain a dictionary with:\n",
    "            # amount_bboxes = {0:3, 1:5}\n",
    "            amount_bboxes = {}\n",
    "            for i in range(num_img):\n",
    "                amount_bboxes[i] = 1\n",
    "            # We then go through each key, val in this dictionary\n",
    "            # and convert to the following (w.r.t same example):\n",
    "            # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n",
    "            for key, val in amount_bboxes.items():\n",
    "                amount_bboxes[key] = torch.zeros(val)\n",
    "\n",
    "            # sort by box probabilities which is index 2\n",
    "            detections.sort(key=lambda x: x[1], reverse=True)\n",
    "            TP = torch.zeros((len(detections)))\n",
    "            FP = torch.zeros((len(detections)))\n",
    "            total_true_bboxes = len(ground_truths)\n",
    "            \n",
    "            # If none exists for this class then we can safely skip\n",
    "            if total_true_bboxes == 0:\n",
    "                continue\n",
    "\n",
    "            for detection_idx, detection in enumerate(detections):\n",
    "                # Only take out the ground_truths that have the same\n",
    "                # training idx as detection\n",
    "                ground_truth_img = [bbox for bbox in ground_truths]\n",
    "\n",
    "                num_gts = len(ground_truth_img)\n",
    "                best_iou = 0\n",
    "                best_gt_idx = 0\n",
    "                for idx, gt in enumerate(ground_truth_img):\n",
    "                    iou_metric = iou(\n",
    "                        torch.tensor(detection[2:]),\n",
    "                        torch.tensor(gt[2:]),\n",
    "                    )\n",
    "\n",
    "                    if iou_metric > best_iou:\n",
    "                        best_iou = iou_metric\n",
    "                        best_gt_idx = idx\n",
    "\n",
    "                if best_iou > iou_threshold:\n",
    "                    # only detect ground truth detection once\n",
    "                    if amount_bboxes[i][best_gt_idx] == 0:\n",
    "                        # true positive and add this bounding box to seen\n",
    "                        TP[detection_idx] = 1\n",
    "                        amount_bboxes[i][best_gt_idx] = 1\n",
    "                    else:\n",
    "                        FP[detection_idx] = 1\n",
    "\n",
    "                # if IOU is lower then the detection is a false positive\n",
    "                else:\n",
    "                    FP[detection_idx] = 1\n",
    "\n",
    "            TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "            FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "            recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
    "            precisions = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)\n",
    "            precisions = torch.cat((torch.tensor([1]), precisions))\n",
    "            recalls = torch.cat((torch.tensor([0]), recalls))\n",
    "            # torch.trapz for numerical integration\n",
    "            average_precisions.append(torch.trapz(precisions, recalls))\n",
    "    print(\"k la\", k)\n",
    "    return sum(average_precisions) / len(average_precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target boxes [[1.0, 1.0, 0.37620192766189575, 0.37860578298568726, 0.286057710647583, 0.5600962042808533]]\n",
      "Target boxes [[0.0, 1.0, 0.46514424681663513, 0.4146634638309479, 0.7475962042808533, 0.5264423489570618]]\n",
      "Detection [[0.0, 0.7336120009422302, 0.39415493607521057, 0.39217254519462585, 0.4283025860786438, 0.33205217123031616], [0.0, 0.7091950178146362, 0.470702588558197, 0.3809179365634918, 0.44609999656677246, 0.38880646228790283], [0.0, 0.6922194361686707, 0.49094563722610474, 0.3764212429523468, 0.3973250389099121, 0.4430702328681946], [0.0, 0.6351394653320312, 0.5077136754989624, 0.3731922507286072, 0.3621458411216736, 0.40502721071243286]]\n",
      "Ground truth [[0.0, 1.0, 0.46514424681663513, 0.4146634638309479, 0.7475962042808533, 0.5264423489570618]]\n",
      "Target boxes [[0.0, 1.0, 0.4531250298023224, 0.41826924681663513, 0.7139423489570618, 0.5240384936332703]]\n",
      "Target boxes [[1.0, 1.0, 0.2343750149011612, 0.4302884638309479, 0.4062500298023224, 0.6826923489570618]]\n",
      "Target boxes [[0.0, 1.0, 0.44110578298568726, 0.3822115659713745, 0.728365421295166, 0.5576923489570618]]\n",
      "Detection [[0.0, 0.6032705307006836, 0.4884350895881653, 0.35868874192237854, 0.24066190421581268, 0.29725566506385803]]\n",
      "Ground truth [[0.0, 1.0, 0.44110578298568726, 0.3822115659713745, 0.728365421295166, 0.5576923489570618]]\n",
      "Target boxes [[1.0, 1.0, 0.39182692766189575, 0.36538463830947876, 0.2836538553237915, 0.5288461446762085]]\n",
      "Target boxes [[0.0, 1.0, 0.4447115659713745, 0.4086538553237915, 0.7307692766189575, 0.5913462042808533]]\n",
      "Target boxes [[1.0, 1.0, 0.5036057829856873, 0.426682710647583, 0.4447115659713745, 0.8197115659713745]]\n",
      "Target boxes [[0.0, 1.0, 0.6442307829856873, 0.45192310214042664, 0.5865384936332703, 0.879807710647583]]\n",
      "Target boxes [[1.0, 1.0, 0.5228365659713745, 0.4375000298023224, 0.4639423191547394, 0.7163462042808533]]\n",
      "Target boxes [[1.0, 1.0, 0.5, 0.41225963830947876, 0.442307710647583, 0.7764423489570618]]\n",
      "Detection [[1.0, 0.6098807454109192, 0.47104379534721375, 0.46621769666671753, 0.1957879513502121, 0.3650442659854889]]\n",
      "Ground truth [[1.0, 1.0, 0.5, 0.41225963830947876, 0.442307710647583, 0.7764423489570618]]\n",
      "Target boxes [[1.0, 1.0, 0.390625, 0.3545673191547394, 0.28125, 0.5649038553237915]]\n",
      "Detection [[1.0, 0.7000569105148315, 0.5288747549057007, 0.37988072633743286, 0.23176008462905884, 0.37907305359840393], [1.0, 0.6014411449432373, 0.4924953281879425, 0.35713788866996765, 0.3065149784088135, 0.37939339876174927], [1.0, 0.6784886121749878, 0.47453734278678894, 0.42889204621315, 0.31883344054222107, 0.8640276193618774], [1.0, 0.6166561841964722, 0.5232956409454346, 0.4456486403942108, 0.5909896492958069, 0.5510748028755188]]\n",
      "Ground truth [[1.0, 1.0, 0.390625, 0.3545673191547394, 0.28125, 0.5649038553237915]]\n",
      "Target boxes [[1.0, 1.0, 0.5036057829856873, 0.4218750298023224, 0.4399038553237915, 0.8052884936332703]]\n",
      "Detection [[1.0, 0.634385883808136, 0.4521235525608063, 0.43342551589012146, 0.2086753249168396, 0.3974727392196655], [1.0, 0.7205219268798828, 0.48949578404426575, 0.4476749897003174, 0.37830936908721924, 0.28933414816856384], [1.0, 0.6240172386169434, 0.5084465146064758, 0.3798588514328003, 0.28191041946411133, 0.36342260241508484], [1.0, 0.6190245151519775, 0.5234047174453735, 0.4447422921657562, 0.5928998589515686, 0.6323532462120056]]\n",
      "Ground truth [[1.0, 1.0, 0.5036057829856873, 0.4218750298023224, 0.4399038553237915, 0.8052884936332703]]\n",
      "Target boxes [[0.0, 1.0, 0.43389424681663513, 0.411057710647583, 0.6899038553237915, 0.5336538553237915]]\n",
      "Target boxes [[1.0, 1.0, 0.5216346383094788, 0.4375000298023224, 0.5432692766189575, 0.759615421295166]]\n",
      "Detection [[1.0, 0.6765510439872742, 0.4719095528125763, 0.46588996052742004, 0.19994477927684784, 0.37915411591529846], [1.0, 0.6587367653846741, 0.49030518531799316, 0.4651656448841095, 0.34778404235839844, 0.33844539523124695], [1.0, 0.6726305484771729, 0.5254602432250977, 0.4450034201145172, 0.4759324789047241, 0.8185741901397705]]\n",
      "Ground truth [[1.0, 1.0, 0.5216346383094788, 0.4375000298023224, 0.5432692766189575, 0.759615421295166]]\n",
      "Target boxes [[1.0, 1.0, 0.39663463830947876, 0.36899039149284363, 0.3413461744785309, 0.540865421295166]]\n",
      "Detection [[1.0, 0.6592531204223633, 0.5290852189064026, 0.38042041659355164, 0.1913435310125351, 0.2910774052143097], [1.0, 0.7155358195304871, 0.4696611166000366, 0.44654643535614014, 0.305611252784729, 0.36340025067329407], [1.0, 0.6141074895858765, 0.4895283877849579, 0.4665893614292145, 0.3034103512763977, 0.28277459740638733], [1.0, 0.658096969127655, 0.5269177556037903, 0.39939185976982117, 0.4955871105194092, 0.5032156705856323], [1.0, 0.6502785682678223, 0.49428698420524597, 0.3981914222240448, 0.4816177487373352, 0.4486744701862335], [1.0, 0.6382209062576294, 0.5093738436698914, 0.4684790372848511, 0.4779911935329437, 0.5874959826469421]]\n",
      "Ground truth [[1.0, 1.0, 0.39663463830947876, 0.36899039149284363, 0.3413461744785309, 0.540865421295166]]\n",
      "Target boxes [[1.0, 1.0, 0.525240421295166, 0.457932710647583, 0.5745192766189575, 0.8100962042808533]]\n",
      "Detection [[1.0, 0.6091296076774597, 0.4910444915294647, 0.37625402212142944, 0.32568228244781494, 0.4166431128978729], [1.0, 0.7867121696472168, 0.5243282318115234, 0.4457301199436188, 0.6354743242263794, 0.8925478458404541], [1.0, 0.7220969200134277, 0.4918557107448578, 0.39699044823646545, 0.3569192886352539, 0.4641689956188202], [1.0, 0.629459798336029, 0.4734710454940796, 0.41515782475471497, 0.34693822264671326, 0.4777374863624573]]\n",
      "Ground truth [[1.0, 1.0, 0.525240421295166, 0.457932710647583, 0.5745192766189575, 0.8100962042808533]]\n",
      "Target boxes [[0.0, 1.0, 0.47836539149284363, 0.3930288553237915, 0.6682692766189575, 0.5168269276618958]]\n",
      "Target boxes [[1.0, 1.0, 0.5120192766189575, 0.4399038553237915, 0.442307710647583, 0.764423131942749]]\n",
      "Detection [[1.0, 0.6344953775405884, 0.39307737350463867, 0.3224909007549286, 0.2741268575191498, 0.24074794352054596], [1.0, 0.6253710389137268, 0.4301739037036896, 0.3424640893936157, 0.19724176824092865, 0.3380524814128876], [1.0, 0.65459144115448, 0.39361217617988586, 0.390276700258255, 0.272459477186203, 0.2702309489250183]]\n",
      "Ground truth [[1.0, 1.0, 0.5120192766189575, 0.4399038553237915, 0.442307710647583, 0.764423131942749]]\n",
      "Target boxes [[1.0, 1.0, 0.4759615659713745, 0.31850963830947876, 0.11057692766189575, 0.10817307978868484]]\n",
      "Detection [[1.0, 0.6298816800117493, 0.4299844801425934, 0.3392789959907532, 0.22678212821483612, 0.29087838530540466]]\n",
      "Ground truth [[1.0, 1.0, 0.4759615659713745, 0.31850963830947876, 0.11057692766189575, 0.10817307978868484]]\n",
      "Target boxes [[1.0, 1.0, 0.37860578298568726, 0.36899039149284363, 0.29086539149284363, 0.5745192766189575]]\n",
      "Target boxes [[0.0, 1.0, 0.4795673191547394, 0.40024039149284363, 0.7716346383094788, 0.5216346383094788]]\n",
      "Target boxes [[1.0, 1.0, 0.390625, 0.36899039149284363, 0.30048078298568726, 0.5120192766189575]]\n",
      "Detection [[1.0, 0.6204330325126648, 0.354149729013443, 0.37261343002319336, 0.20287691056728363, 0.41552427411079407], [1.0, 0.6048629283905029, 0.47027552127838135, 0.37254104018211365, 0.3034495413303375, 0.32806894183158875]]\n",
      "Ground truth [[1.0, 1.0, 0.390625, 0.36899039149284363, 0.30048078298568726, 0.5120192766189575]]\n",
      "Target boxes [[1.0, 1.0, 0.3858173191547394, 0.3665865659713745, 0.286057710647583, 0.5697115659713745]]\n",
      "Detection [[1.0, 0.6092786192893982, 0.374609112739563, 0.3903121054172516, 0.1814710944890976, 0.40208661556243896]]\n",
      "Ground truth [[1.0, 1.0, 0.3858173191547394, 0.3665865659713745, 0.286057710647583, 0.5697115659713745]]\n",
      "Target boxes [[1.0, 1.0, 0.3834134638309479, 0.37379810214042664, 0.29567310214042664, 0.5649038553237915]]\n",
      "Detection [[1.0, 0.6567333340644836, 0.3948182165622711, 0.40967822074890137, 0.1832868754863739, 0.3975619077682495], [1.0, 0.6381237506866455, 0.35581091046333313, 0.3920130729675293, 0.19669711589813232, 0.3771876096725464], [1.0, 0.6146114468574524, 0.41156554222106934, 0.41006553173065186, 0.27699971199035645, 0.244933620095253]]\n",
      "Ground truth [[1.0, 1.0, 0.3834134638309479, 0.37379810214042664, 0.29567310214042664, 0.5649038553237915]]\n",
      "Target boxes [[0.0, 1.0, 0.4603365659713745, 0.40985578298568726, 0.6947115659713745, 0.5456730723381042]]\n",
      "Target boxes [[1.0, 1.0, 0.390625, 0.359375, 0.3100961744785309, 0.59375]]\n",
      "Target boxes [[0.0, 1.0, 0.45432692766189575, 0.38100963830947876, 0.7307692766189575, 0.540865421295166]]\n",
      "Target boxes [[1.0, 1.0, 0.4603365659713745, 0.587740421295166, 0.29567310214042664, 0.29086539149284363]]\n",
      "Detection [[1.0, 0.6709973812103271, 0.3377680778503418, 0.3780151605606079, 0.2469698190689087, 0.3717998266220093], [1.0, 0.6510800719261169, 0.37444931268692017, 0.3425440192222595, 0.3172072470188141, 0.28847452998161316]]\n",
      "Ground truth [[1.0, 1.0, 0.4603365659713745, 0.587740421295166, 0.29567310214042664, 0.29086539149284363]]\n",
      "Target boxes [[1.0, 1.0, 0.3834134638309479, 0.3617788553237915, 0.29567310214042664, 0.5745192766189575]]\n",
      "Target boxes [[1.0, 1.0, 0.375, 0.38100963830947876, 0.3125, 0.5456730723381042]]\n",
      "Target boxes [[1.0, 1.0, 0.5264423489570618, 0.45432692766189575, 0.5769230723381042, 0.6394230723381042]]\n",
      "Target boxes [[0.0, 1.0, 0.4302884638309479, 0.4062500298023224, 0.7163462042808533, 0.5240384936332703]]\n",
      "Target boxes [[0.0, 1.0, 0.46514424681663513, 0.4194711744785309, 0.7668269276618958, 0.5793269276618958]]\n",
      "Target boxes [[1.0, 1.0, 0.38942310214042664, 0.35336539149284363, 0.2740384638309479, 0.5769230723381042]]\n",
      "k la 14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.1786)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_average_precision(pred_boxes, target_boxes, num_classes=2,num_img=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = calculate_mAP(pred_boxes, target_boxes, num_classes=3, num_img=len(target_boxes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 6)\n"
     ]
    }
   ],
   "source": [
    "a= [[0.0, 0.7336120009422302, 0.39415493607521057, 0.39217254519462585, 0.4283025860786438, 0.33205217123031616], [0.0, 0.7091950178146362, 0.470702588558197, 0.3809179365634918, 0.44609999656677246, 0.38880646228790283], [0.0, 0.6922194361686707, 0.49094563722610474, 0.3764212429523468, 0.3973250389099121, 0.4430702328681946], [0.0, 0.6351394653320312, 0.5077136754989624, 0.3731922507286072, 0.3621458411216736, 0.40502721071243286]]\n",
    "print(np.asarray(a).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "a = []\n",
    "if a == []:\n",
    "    print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projectAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
