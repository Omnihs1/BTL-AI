{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import YOLOv3, ScalePrediction\n",
    "from library import *\n",
    "from config import learning_rate, checkpoint_file, ANCHORS, device, batch_size, s\n",
    "from utils import load_checkpoint, convert_cells_to_bboxes, plot_image, save_checkpoint\n",
    "from dataset import Dataset\n",
    "from augment import test_transform\n",
    "from loss import YOLOLoss\n",
    "from metrics import nms\n",
    "import multiprocessing\n",
    "from augment import train_transform\n",
    "from train import training_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Loading checkpoint\n"
     ]
    }
   ],
   "source": [
    "# Setting the load_model to True\n",
    "load_model = True\n",
    "save_model = True\n",
    "# Defining the model, optimizer, loss function and scaler\n",
    "model = YOLOv3(num_classes = 20).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "loss_fn = YOLOLoss()\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "epochs = 1\n",
    "batch_size = 8\n",
    "checkpoint_file = \"save model/checkpoint.pth.tar\"\n",
    "if __name__ == '__main__':\n",
    "    multiprocessing.freeze_support()\n",
    "    # Loading the checkpoint\n",
    "    if load_model:\n",
    "        load_checkpoint(checkpoint_file, model, optimizer, learning_rate)  \n",
    "    \n",
    "    \n",
    "    num_classes = 4\n",
    "    new_model = YOLOv3(num_classes = 4).to(device)\n",
    "    state_dict_source = model.state_dict()\n",
    "    model = model.to('cpu')\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict_target = new_model.state_dict()\n",
    "for i, layer_name in enumerate(state_dict_target.keys()):\n",
    "    if i > 357:\n",
    "        break\n",
    "    state_dict_target[layer_name] = state_dict_source[layer_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanh cong!\n"
     ]
    }
   ],
   "source": [
    "new_model.load_state_dict(state_dict_target)    \n",
    "print(\"Thanh cong!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in new_model.layers[:10].parameters():\n",
    "     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = True\n",
    "save_model = True\n",
    "new_model = YOLOv3(num_classes = 4).to(device)\n",
    "new_optimizer = optim.Adam(new_model.parameters(), lr = learning_rate)\n",
    "loss_fn = YOLOLoss()\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dùng loading checkpoint khi muốn thêm dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Loading checkpoint\n"
     ]
    }
   ],
   "source": [
    "checkpoint_file = \"save model/checkpoint_custom_final.pth.tar\"\n",
    "if load_model:\n",
    "    load_checkpoint(checkpoint_file, new_model, new_optimizer, learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(loader, model, optimizer, loss_fn, scaler, scaled_anchors):\n",
    "    # Creating a progress bar\n",
    "    progress_bar = tqdm(loader, leave=True)\n",
    "  \n",
    "    # Initializing a list to store the losses\n",
    "    losses = []\n",
    "    all_box_loss = []\n",
    "    all_object_loss = []\n",
    "    all_no_object_loss = []\n",
    "    all_class_loss = []\n",
    "  \n",
    "    # Iterating over the training data\n",
    "    for _, (x, y) in enumerate(progress_bar):\n",
    "        x = x.to(device)\n",
    "        y0, y1, y2 = (\n",
    "            y[0].to(device),\n",
    "            y[1].to(device),\n",
    "            y[2].to(device),\n",
    "        )\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Getting the model predictions\n",
    "            outputs = model(x)\n",
    "            # Calculating the loss at each scale\n",
    "            loss = (\n",
    "                  loss_fn(outputs[0], y0, scaled_anchors[0])\n",
    "                + loss_fn(outputs[1], y1, scaled_anchors[1])\n",
    "                + loss_fn(outputs[2], y2, scaled_anchors[2])\n",
    "            )\n",
    "            a = loss_fn(outputs[0], y0, scaled_anchors[0], mode = \"box\") \\\n",
    "                + loss_fn(outputs[1], y1, scaled_anchors[1], mode = \"box\") \\\n",
    "                + loss_fn(outputs[2], y2, scaled_anchors[2], mode = \"box\")\n",
    "            all_box_loss.append(a.detach().cpu().numpy())\n",
    "            b = loss_fn(outputs[0], y0, scaled_anchors[0], mode = \"object\") \\\n",
    "                                + loss_fn(outputs[1], y1, scaled_anchors[1], mode = \"object\") \\\n",
    "                                + loss_fn(outputs[2], y2, scaled_anchors[2], mode = \"object\")\n",
    "            all_object_loss.append(b.detach().cpu().numpy())\n",
    "            c = loss_fn(outputs[0], y0, scaled_anchors[0], mode = \"no object\") \\\n",
    "                                + loss_fn(outputs[1], y1, scaled_anchors[1], mode = \"no object\") \\\n",
    "                                + loss_fn(outputs[2], y2, scaled_anchors[2], mode = \"no object\")\n",
    "            all_no_object_loss.append(c.detach().cpu().numpy())\n",
    "            d = loss_fn(outputs[0], y0, scaled_anchors[0], mode = \"class\") \\\n",
    "                                + loss_fn(outputs[1], y1, scaled_anchors[1], mode = \"class\") \\\n",
    "                                + loss_fn(outputs[2], y2, scaled_anchors[2], mode = \"class\")\n",
    "            all_class_loss.append(d.detach().cpu().numpy())\n",
    "        # Add the loss to the list\n",
    "        losses.append(loss.item())\n",
    "  \n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "  \n",
    "        # Backpropagate the loss\n",
    "        scaler.scale(loss).backward()\n",
    "  \n",
    "        # Optimization step\n",
    "        scaler.step(optimizer)\n",
    "  \n",
    "        # Update the scaler for next iteration\n",
    "        scaler.update()\n",
    "  \n",
    "        # update progress bar with loss\n",
    "        mean_loss = sum(losses) / len(losses)\n",
    "        progress_bar.set_postfix(loss=mean_loss)\n",
    "\n",
    "    \n",
    "    return losses, all_box_loss, all_object_loss, all_no_object_loss, all_class_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:15<00:00,  2.39it/s, loss=0.368]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4102,)\n",
      "(4102,)\n",
      "(4102,)\n",
      "(4102,)\n",
      "(4102,)\n",
      "==> Saving checkpoint\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:14<00:00,  2.50it/s, loss=0.375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4138,)\n",
      "(4138,)\n",
      "(4138,)\n",
      "(4138,)\n",
      "(4138,)\n",
      "==> Saving checkpoint\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:15<00:00,  2.40it/s, loss=0.354]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4174,)\n",
      "(4174,)\n",
      "(4174,)\n",
      "(4174,)\n",
      "(4174,)\n",
      "==> Saving checkpoint\n",
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:16<00:00,  2.14it/s, loss=0.332]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4210,)\n",
      "(4210,)\n",
      "(4210,)\n",
      "(4210,)\n",
      "(4210,)\n",
      "==> Saving checkpoint\n",
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:14<00:00,  2.42it/s, loss=0.366]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4246,)\n",
      "(4246,)\n",
      "(4246,)\n",
      "(4246,)\n",
      "(4246,)\n",
      "==> Saving checkpoint\n",
      "Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:15<00:00,  2.38it/s, loss=0.332]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4282,)\n",
      "(4282,)\n",
      "(4282,)\n",
      "(4282,)\n",
      "(4282,)\n",
      "==> Saving checkpoint\n",
      "Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:14<00:00,  2.48it/s, loss=0.337]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4318,)\n",
      "(4318,)\n",
      "(4318,)\n",
      "(4318,)\n",
      "(4318,)\n",
      "==> Saving checkpoint\n",
      "Epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:16<00:00,  2.24it/s, loss=0.333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4354,)\n",
      "(4354,)\n",
      "(4354,)\n",
      "(4354,)\n",
      "(4354,)\n",
      "==> Saving checkpoint\n",
      "Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:14<00:00,  2.42it/s, loss=0.377]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4390,)\n",
      "(4390,)\n",
      "(4390,)\n",
      "(4390,)\n",
      "(4390,)\n",
      "==> Saving checkpoint\n",
      "Epoch: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:14<00:00,  2.44it/s, loss=0.357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4426,)\n",
      "(4426,)\n",
      "(4426,)\n",
      "(4426,)\n",
      "(4426,)\n",
      "==> Saving checkpoint\n",
      "Epoch: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:14<00:00,  2.41it/s, loss=0.322]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4462,)\n",
      "(4462,)\n",
      "(4462,)\n",
      "(4462,)\n",
      "(4462,)\n",
      "==> Saving checkpoint\n",
      "Epoch: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:14<00:00,  2.47it/s, loss=0.35] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4498,)\n",
      "(4498,)\n",
      "(4498,)\n",
      "(4498,)\n",
      "(4498,)\n",
      "==> Saving checkpoint\n",
      "Epoch: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:14<00:00,  2.42it/s, loss=0.334]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4534,)\n",
      "(4534,)\n",
      "(4534,)\n",
      "(4534,)\n",
      "(4534,)\n",
      "==> Saving checkpoint\n",
      "Epoch: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:14<00:00,  2.48it/s, loss=0.326]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4570,)\n",
      "(4570,)\n",
      "(4570,)\n",
      "(4570,)\n",
      "(4570,)\n",
      "==> Saving checkpoint\n",
      "Epoch: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:14<00:00,  2.42it/s, loss=0.309]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4606,)\n",
      "(4606,)\n",
      "(4606,)\n",
      "(4606,)\n",
      "(4606,)\n",
      "==> Saving checkpoint\n",
      "Epoch: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:14<00:00,  2.48it/s, loss=0.311]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4642,)\n",
      "(4642,)\n",
      "(4642,)\n",
      "(4642,)\n",
      "(4642,)\n",
      "==> Saving checkpoint\n",
      "Epoch: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:14<00:00,  2.46it/s, loss=0.29] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4678,)\n",
      "(4678,)\n",
      "(4678,)\n",
      "(4678,)\n",
      "(4678,)\n",
      "==> Saving checkpoint\n",
      "Epoch: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:14<00:00,  2.43it/s, loss=0.306]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4714,)\n",
      "(4714,)\n",
      "(4714,)\n",
      "(4714,)\n",
      "(4714,)\n",
      "==> Saving checkpoint\n",
      "Epoch: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:15<00:00,  2.37it/s, loss=0.288]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4750,)\n",
      "(4750,)\n",
      "(4750,)\n",
      "(4750,)\n",
      "(4750,)\n",
      "==> Saving checkpoint\n",
      "Epoch: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:14<00:00,  2.44it/s, loss=0.276]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4786,)\n",
      "(4786,)\n",
      "(4786,)\n",
      "(4786,)\n",
      "(4786,)\n",
      "==> Saving checkpoint\n"
     ]
    }
   ],
   "source": [
    "# # Defining the train dataset\n",
    "epochs = 20\n",
    "# train_dataset = Dataset(\n",
    "#     csv_file=\"more data/train_3.csv\",\n",
    "#     image_dir=\"more data/dataset_resized_2/dataset_resized_2/Img\",\n",
    "#     label_dir=\"more data/dataset_resized_2/dataset_resized_2/Label\",\n",
    "#     anchors=ANCHORS,\n",
    "#     transform=train_transform,\n",
    "#     num_classes = 4\n",
    "# )\n",
    "train_dataset = Dataset(\n",
    "    csv_file=\"custom data/train.csv\",\n",
    "    image_dir=\"custom data/dataset_resized/dataset_resized/Img\",\n",
    "    label_dir=\"custom data/dataset_resized/dataset_resized/Label\",\n",
    "    anchors=ANCHORS,\n",
    "    transform=train_transform,\n",
    "    num_classes = 4\n",
    ")\n",
    "\n",
    "# # Defining the train data loader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = 4,\n",
    "    num_workers = 2,\n",
    "    shuffle = True,\n",
    "    pin_memory = True,\n",
    ")\n",
    "# # Scaling the anchors\n",
    "scaled_anchors = (\n",
    "    torch.tensor(ANCHORS) * \n",
    "    torch.tensor(s).unsqueeze(1).unsqueeze(1).repeat(1,3,2)\n",
    ").to(device)\n",
    "\n",
    "# print(scaled_anchors.shape)\n",
    "\n",
    "# # Training the model\n",
    "for e in range(1, epochs+1):\n",
    "    print(\"Epoch:\", e)\n",
    "    losses, all_box_loss, all_object_loss, all_no_object_loss, all_class_loss = training_loop(train_loader, new_model, new_optimizer, loss_fn, scaler, scaled_anchors)\n",
    "    a = np.load(\"all_loss.npy\")\n",
    "    b = np.load(\"all_box_loss.npy\")\n",
    "    c = np.load(\"all_object_loss.npy\")\n",
    "    d = np.load(\"all_no_object_loss.npy\")\n",
    "    e = np.load(\"all_no_object_loss.npy\")\n",
    "    losses = np.concatenate((a, losses), axis=0)\n",
    "    print(losses.shape)\n",
    "    all_box_loss = np.concatenate((b, all_box_loss), axis=0)\n",
    "    print(all_box_loss.shape)\n",
    "    all_object_loss = np.concatenate((c, all_object_loss), axis=0)\n",
    "    print(all_object_loss.shape)\n",
    "    all_no_object_loss = np.concatenate((d, all_no_object_loss), axis=0)\n",
    "    print(all_no_object_loss.shape)\n",
    "    all_class_loss = np.concatenate((e, all_class_loss), axis=0)\n",
    "    print(all_class_loss.shape)\n",
    "    np.save(\"all_loss.npy\", losses)\n",
    "    np.save(\"all_box_loss.npy\", all_box_loss)\n",
    "    np.save(\"all_object_loss.npy\", all_object_loss)\n",
    "    np.save(\"all_no_object_loss.npy\", all_no_object_loss)\n",
    "    np.save(\"all_class_loss.npy\", all_class_loss)\n",
    "    # Saving the model\n",
    "    if save_model:\n",
    "        save_checkpoint(new_model, new_optimizer, filename=f\"save model/checkpoint_custom_final.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projectAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
